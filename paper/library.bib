

@article{Lecun2015,
  author =        {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal =       {Nature},
  number =        {7553},
  pages =         {436--444},
  title =         {{Deep learning}},
  volume =        {521},
  year =          {2015},
  abstract =      {Deep learning allows computational models that are
                   composed of multiple processing layers to learn
                   representations of data with multiple levels of
                   abstraction. These methods have dramatically improved
                   the state-of-the-art in speech recognition, visual
                   object recognition, object detection and many other
                   domains such as drug discovery and genomics. Deep
                   learning discovers intricate structure in large data
                   sets by using the backpropagation algorithm to
                   indicate how a machine should change its internal
                   parameters that are used to compute the
                   representation in each layer from the representation
                   in the previous layer. Deep convolutional nets have
                   brought about breakthroughs in processing images,
                   video, speech and audio, whereas recurrent nets have
                   shone light on sequential data such as text and
                   speech.},
  doi =           {10.1038/nature14539},
  isbn =          {9780521835688},
  issn =          {14764687},
}

@article{Schmidhuber2015,
  author =        {Schmidhuber, J{\"{u}}rgen},
  journal =       {Neural Networks},
  pages =         {85--117},
  publisher =     {Elsevier Ltd},
  title =         {{Deep Learning in neural networks: An overview}},
  volume =        {61},
  year =          {2015},
  abstract =      {In recent years, deep artificial neural networks
                   (including recurrent ones) have won numerous contests
                   in pattern recognition and machine learning. This
                   historical survey compactly summarizes relevant work,
                   much of it from the previous millennium. Shallow and
                   Deep Learners are distinguished by the depth of their
                   credit assignment paths, which are chains of possibly
                   learnable, causal links between actions and effects.
                   I review deep supervised learning (also
                   recapitulating the history of backpropagation),
                   unsupervised learning, reinforcement learning {\&}
                   evolutionary computation, and indirect search for
                   short programs encoding deep and large networks.},
  doi =           {10.1016/j.neunet.2014.09.003},
  isbn =          {0893-6080},
  issn =          {18792782},
  url =           {http://dx.doi.org/10.1016/j.neunet.2014.09.003},
}

@article{Prieto2016,
  author =        {Prieto, Alberto and Prieto, Beatriz and
                   Ortigosa, Eva Martinez and Ros, Eduardo and
                   Pelayo, Francisco and Ortega, Julio and
                   Rojas, Ignacio},
  journal =       {Neurocomputing},
  title =         {{Neural networks: An overview of early research,
                   current frameworks and new challenges}},
  year =          {2016},
  abstract =      {This paper presents a comprehensive overview of
                   modelling, simulation and implementation of neural
                   networks, taking into account that two aims have
                   emerged in this area: the improvement of our
                   understanding of the behaviour of the nervous system
                   and the need to find inspiration from it to build
                   systems with the advantages provided by nature to
                   perform certain relevant tasks. The development and
                   evolution of different topics related to neural
                   networks is described (simulators, implementations,
                   and real-world applications) showing that the field
                   has acquired maturity and consolidation, proven by
                   its competitiveness in solving real-world problems.
                   The paper also shows how, over time, artificial
                   neural networks have contributed to fundamental
                   concepts at the birth and development of other
                   disciplines such as Computational Neuroscience,
                   Neuro-engineering, Computational Intelligence and
                   Machine Learning. A better understanding of the human
                   brain is considered one of the challenges of this
                   century, and to achieve it, as this paper goes on to
                   describe, several important national and
                   multinational projects and initiatives are marking
                   the way to follow in neural-network research.},
  doi =           {10.1016/j.neucom.2016.06.014},
  issn =          {18728286},
}

@book{Goodfellow2016,
  author =        {Goodfellow, Ian and Bengio, Yoshua and
                   Courville, Aaron},
  publisher =     {MIT Press},
  title =         {{Deep Learning}},
  year =          {2016},
  url =           {http://www.deeplearningbook.org},
}

@article{Liu2017,
  author =        {Liu, Weibo and Wang, Zidong and Liu, Xiaohui and
                   Zeng, Nianyin and Liu, Yurong and Alsaadi, Fuad E.},
  journal =       {Neurocomputing},
  title =         {{A survey of deep neural network architectures and
                   their applications}},
  year =          {2017},
  abstract =      {Since the proposal of a fast learning algorithm for
                   deep belief networks in 2006, the deep learning
                   techniques have drawn ever-increasing research
                   interests because of their inherent capability of
                   overcoming the drawback of traditional algorithms
                   dependent on hand-designed features. Deep learning
                   approaches have also been found to be suitable for
                   big data analysis with successful applications to
                   computer vision, pattern recognition, speech
                   recognition, natural language processing, and
                   recommendation systems. In this paper, we discuss
                   some widely-used deep learning architectures and
                   their practical applications. An up-to-date overview
                   is provided on four deep learning architectures,
                   namely, autoencoder, convolutional neural network,
                   deep belief network, and restricted Boltzmann
                   machine. Different types of deep neural networks are
                   surveyed and recent progresses are summarized.
                   Applications of deep learning techniques on some
                   selected areas (speech recognition, pattern
                   recognition and computer vision) are highlighted. A
                   list of future research topics are finally given with
                   clear justifications.},
  doi =           {10.1016/j.neucom.2016.12.038},
  isbn =          {0925-2312},
  issn =          {18728286},
}

@article{ling2016reynolds,
  author =        {Ling, Julia and Kurzawski, Andrew and
                   Templeton, Jeremy},
  journal =       {Journal of Fluid Mechanics},
  pages =         {155--166},
  publisher =     {Cambridge University Press},
  title =         {Reynolds averaged turbulence modelling using deep
                   neural networks with embedded invariance},
  volume =        {807},
  year =          {2016},
}

@inproceedings{duraisamy2015new,
  author =        {Duraisamy, Karthikeyan and Zhang, Ze J and
                   Singh, Anand Pratap},
  booktitle =     {53rd AIAA Aerospace Sciences Meeting},
  pages =         {1284},
  title =         {New approaches in turbulence and transition modeling
                   using data-driven techniques},
  year =          {2015},
}

@article{duraisamy2019turbulence,
  author =        {Duraisamy, Karthik and Iaccarino, Gianluca and
                   Xiao, Heng},
  journal =       {Annual Review of Fluid Mechanics},
  pages =         {357--377},
  publisher =     {Annual Reviews},
  title =         {Turbulence modeling in the age of data},
  volume =        {51},
  year =          {2019},
}

@inproceedings{zinkevich2010parallelized,
  author =        {Zinkevich, Martin and Weimer, Markus and Li, Lihong and
                   Smola, Alex J},
  booktitle =     {Advances in neural information processing systems},
  pages =         {2595--2603},
  title =         {Parallelized stochastic gradient descent},
  year =          {2010},
}

@incollection{bottou2010large,
  author =        {Bottou, L{\'e}on},
  booktitle =     {Proceedings of COMPSTAT'2010},
  pages =         {177--186},
  publisher =     {Springer},
  title =         {Large-scale machine learning with stochastic gradient
                   descent},
  year =          {2010},
}

@article{kingma2014adam,
  author =        {Kingma, Diederik P and Ba, Jimmy},
  journal =       {arXiv preprint arXiv:1412.6980},
  title =         {Adam: A method for stochastic optimization},
  year =          {2014},
}

@article{tieleman2012lecture,
  author =        {Tieleman, Tijmen and Hinton, Geoffrey},
  journal =       {COURSERA: Neural networks for machine learning},
  number =        {2},
  pages =         {26--31},
  title =         {Lecture 6.5-rmsprop: Divide the gradient by a running
                   average of its recent magnitude},
  volume =        {4},
  year =          {2012},
}

@article{duchi2011adaptive,
  author =        {Duchi, John and Hazan, Elad and Singer, Yoram},
  journal =       {Journal of Machine Learning Research},
  number =        {Jul},
  pages =         {2121--2159},
  title =         {Adaptive subgradient methods for online learning and
                   stochastic optimization},
  volume =        {12},
  year =          {2011},
}

@article{Cohen2017,
  author =        {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and
                   van Schaik, Andr{\'{e}}},
  month =         {feb},
  title =         {{EMNIST: an extension of MNIST to handwritten
                   letters}},
  year =          {2017},
  abstract =      {The MNIST dataset has become a standard benchmark for
                   learning, classification and computer vision systems.
                   Contributing to its widespread adoption are the
                   understandable and intuitive nature of the task, its
                   relatively small size and storage requirements and
                   the accessibility and ease-of-use of the database
                   itself. The MNIST database was derived from a larger
                   dataset known as the NIST Special Database 19 which
                   contains digits, uppercase and lowercase handwritten
                   letters. This paper introduces a variant of the full
                   NIST dataset, which we have called Extended MNIST
                   (EMNIST), which follows the same conversion paradigm
                   used to create the MNIST dataset. The result is a set
                   of datasets that constitute a more challenging
                   classification tasks involving letters and digits,
                   and that shares the same image structure and
                   parameters as the original MNIST task, allowing for
                   direct compatibility with all existing classifiers
                   and systems. Benchmark results are presented along
                   with a validation of the conversion process through
                   the comparison of the classification results on
                   converted NIST digits and the MNIST digits.},
  url =           {http://arxiv.org/abs/1702.05373},
}

@article{Kingma2014,
  author =        {Kingma, Diederik P. and Ba, Jimmy},
  month =         {dec},
  title =         {{Adam: A Method for Stochastic Optimization}},
  year =          {2014},
  abstract =      {We introduce Adam, an algorithm for first-order
                   gradient-based optimization of stochastic objective
                   functions, based on adaptive estimates of lower-order
                   moments. The method is straightforward to implement,
                   is computationally efficient, has little memory
                   requirements, is invariant to diagonal rescaling of
                   the gradients, and is well suited for problems that
                   are large in terms of data and/or parameters. The
                   method is also appropriate for non-stationary
                   objectives and problems with very noisy and/or sparse
                   gradients. The hyper-parameters have intuitive
                   interpretations and typically require little tuning.
                   Some connections to related algorithms, on which Adam
                   was inspired, are discussed. We also analyze the
                   theoretical convergence properties of the algorithm
                   and provide a regret bound on the convergence rate
                   that is comparable to the best known results under
                   the online convex optimization framework. Empirical
                   results demonstrate that Adam works well in practice
                   and compares favorably to other stochastic
                   optimization methods. Finally, we discuss AdaMax, a
                   variant of Adam based on the infinity norm.},
  url =           {http://arxiv.org/abs/1412.6980},
}

@article{Rogallo1984,
  author =        {Rogallo, R S and Moin, P},
  journal =       {Annu. Rev. Fluid Mech.},
  month =         {jan},
  number =        {1},
  pages =         {99--137},
  title =         {{Numerical Simulation of Turbulent Flows}},
  volume =        {16},
  year =          {1984},
  doi =           {10.1146/annurev.fl.16.010184.000531},
  issn =          {0066-4189},
  url =           {http://www.annualreviews.org/doi/10.1146/
                   annurev.fl.16.010184.000531},
}

@article{Lesieur1996,
  author =        {Lesieur, Marcel},
  journal =       {Annu. Rev. Fluid Mech.},
  month =         {jan},
  number =        {1},
  pages =         {45--82},
  title =         {{New Trends in Large-Eddy Simulations of Turbulence}},
  volume =        {28},
  year =          {1996},
  abstract =      {The paper presents large-eddy simulation (LES)
                   formalism, along with the various subgrid-scale
                   models developed since Smagorinskys model. We show
                   how Kraichnans spectral eddy viscosity may be
                   implemented in physical space, yielding the
                   structure-function model. Recent developments of this
                   model that allow the eddy viscosity to be inhibited
                   in transitional regions are discussed. We present a
                   dynamic procedure, where a double filtering allows
                   one to dynamically determine the subgrid-scale model
                   constants. The importance of backscatter effects is
                   discussed. Alternatives to the eddy-viscosity
                   assumption, such as scale- similarity models, are
                   considered. Pseudo-direct simulations in which
                   numerical diffusion replaces subgrid transfers are
                   mentioned. Various applications of LES to
                   incompressible and compressible turbulent flows are
                   given, with an emphasis on the generation of coherent
                   vortices.},
  doi =           {10.1146/annurev.fluid.28.1.45},
  isbn =          {0066-4189},
  issn =          {00664189},
  url =           {http://fluid.annualreviews.org/cgi/doi/10.1146/
                   annurev.fluid.28.1.45},
}

@article{Piomelli1999,
  author =        {Piomelli, U.},
  journal =       {Prog. Aerosp. Sci.},
  month =         {may},
  number =        {4},
  pages =         {335--362},
  title =         {{Large-eddy simulation: achievements and challenges}},
  volume =        {35},
  year =          {1999},
  abstract =      {In this paper, the present state of the large-eddy
                   simulation (LES) technique is discussed. Modelling
                   and numerical issues that are under study will be
                   described, and results of state-of-the-art
                   calculations will be presented to highlight the
                   response of the subgrid-scale models to important
                   features of the flow field. Finally, some challenges
                   for research and applications of LES in the near
                   future will be discussed.},
  doi =           {10.1016/S0376-0421(98)00014-1},
  isbn =          {0376-0421},
  issn =          {03760421},
  url =           {http://linkinghub.elsevier.com/retrieve/pii/
                   S0376042198000141},
}

@article{Meneveau2000,
  author =        {Meneveau, Charles and Katz, Joseph},
  journal =       {Annu. Rev. Fluid Mech.},
  month =         {jan},
  number =        {1},
  pages =         {1--32},
  title =         {{Scale-Invariance and Turbulence Models for
                   Large-Eddy Simulation}},
  volume =        {32},
  year =          {2000},
  doi =           {10.1146/annurev.fluid.32.1.1},
  issn =          {0066-4189},
  url =           {http://www.annualreviews.org/doi/10.1146/
                   annurev.fluid.32.1.1},
}

@article{Smagorinsky1963,
  author =        {SMAGORINSKY, J.},
  journal =       {Mon. Weather Rev.},
  month =         {mar},
  number =        {3},
  pages =         {99--164},
  title =         {{GENERAL CIRCULATION EXPERIMENTS WITH THE PRIMITIVE
                   EQUATIONS}},
  volume =        {91},
  year =          {1963},
  abstract =      {Abstract An extended period numerical integration of
                   a baroclinic primitive equation model has been made
                   for the simulation and the study of the dynamics of
                   the atmosphere's general circulation. The solution
                   corresponding to external gravitational propagation
                   is filtered by requiring the vertically integrated
                   divergence to vanish identically. The vertical
                   structure permits as dependent variables the
                   horizontal wind at two internal levels and a single
                   temperature, with the static stability entering as a
                   parameter. The incoming radiation is a function of
                   latitude only corresponding to the annual mean, and
                   the outgoing radiation is taken to be a function of
                   the local temperature. With the requirement for
                   thermal equilibrium, the domain mean temperature is
                   specified as a parameter. The role of condensation is
                   taken into account only as it effectively reduces the
                   static stability. All other external sources and
                   sinks of heat are assumed to balance each other
                   locally, and are thus omitted. The kinematics are
                   th...},
  doi =           {10.1175/1520-0493(1963)091<0099:GCEWTP>2.3.CO;2},
  isbn =          {1520-0493},
  issn =          {0027-0644},
  url =           {http://journals.ametsoc.org/doi/abs/10.1175/1520-
                   0493(1963)091{\%}3C0099:GCEWTP{\%}3E2.3.CO;2 http://
                   journals.ametsoc.org/doi/abs/10.1175/1520-
  0493{\%}281963{\%}29091{\%}3C0099{\%}3AGCEWTP{\%}3E2.3.CO{\%}3B2},
}

@article{Ling2016,
  author =        {Ling, Julia and Kurzawski, Andrew and
                   Templeton, Jeremy},
  journal =       {J. Fluid Mech.},
  title =         {{Reynolds averaged turbulence modelling using deep
                   neural networks with embedded invariance}},
  year =          {2016},
  abstract =      {{\textless}p{\textgreater}There exists significant
                   demand for improved Reynolds-averaged Navier–Stokes
                   (RANS) turbulence models that are informed by and can
                   represent a richer set of turbulence physics. This
                   paper presents a method of using deep neural networks
                   to learn a model for the Reynolds stress anisotropy
                   tensor from high-fidelity simulation data. A novel
                   neural network architecture is proposed which uses a
                   multiplicative layer with an invariant tensor basis
                   to embed Galilean invariance into the predicted
                   anisotropy tensor. It is demonstrated that this
                   neural network architecture provides improved
                   prediction accuracy compared with a generic neural
                   network architecture that does not embed this
                   invariance property. The Reynolds stress anisotropy
                   predictions of this invariant neural network are
                   propagated through to the velocity field for two test
                   cases. For both test cases, significant improvement
                   versus baseline RANS linear eddy viscosity and
                   nonlinear eddy viscosity models is
                   demonstrated.{\textless}/p{\textgreater}},
  doi =           {10.1017/jfm.2016.615},
  isbn =          {0028-3932 (Print)$\backslash$r0028-3932 (Linking)},
  issn =          {14697645},
}

@article{Maulik2017,
  author =        {Maulik, R. and San, O.},
  journal =       {J. Fluid Mech.},
  month =         {nov},
  pages =         {151--181},
  title =         {{A neural network approach for the blind
                   deconvolution of turbulent flows}},
  volume =        {831},
  year =          {2017},
  abstract =      {We present a single-layer feed-forward artificial
                   neural network architecture trained through a
                   supervised learning approach for the deconvolution of
                   flow variables from their coarse-grained computations
                   such as those encountered in large eddy simulations.
                   We stress that the deconvolution procedure proposed
                   in this investigation is blind, i.e. the deconvolved
                   field is computed without any pre-existing
                   information about the filtering procedure or kernel.
                   This may be conceptually contrasted to the celebrated
                   approximate deconvolution approaches where a filter
                   shape is predefined for an iterative deconvolution
                   process. We demonstrate that the proposed blind
                   deconvolution network performs exceptionally well in
                   the a priori testing of two-dimensional Kraichnan,
                   three-dimensional Kolmogorov and compressible
                   stratified turbulence test cases, and shows promise
                   in forming the backbone of a physics-augmented
                   data-driven closure for the Navier–Stokes
                   equations.},
  doi =           {10.1017/jfm.2017.637},
  issn =          {0022-1120},
  url =           {https://www.cambridge.org/core/product/identifier/
                   S0022112017006371/type/journal{\_}article},
}

@article{lee2015direct,
  author =        {Lee, Myoungkyu and Moser, Robert D},
  journal =       {Journal of Fluid Mechanics},
  pages =         {395--415},
  publisher =     {Cambridge University Press},
  title =         {Direct numerical simulation of turbulent channel flow
                   up to $\mathit{Re}_\tau\approx 5200$},
  volume =        {774},
  year =          {2015},
}

@inproceedings{lee2013petascale,
  author =        {Lee, Myoungkyu and Malaya, Nicholas and
                   Moser, Robert D},
  booktitle =     {Proceedings of the International Conference on High
                   Performance Computing, Networking, Storage and
                   Analysis},
  organization =  {ACM},
  pages =         {61},
  title =         {Petascale direct numerical simulation of turbulent
                   channel flow on up to 786k cores},
  year =          {2013},
}

@article{lee2014experiences,
  author =        {Lee, Myoungkyu and Ulerich, Rhys and Malaya, Nicholas and
                   Moser, Robert D},
  journal =       {Computing in Science \& Engineering},
  number =        {5},
  pages =         {24--31},
  publisher =     {IEEE},
  title =         {Experiences from leadership computing in simulations
                   of turbulent fluid flows},
  volume =        {16},
  year =          {2014},
}

@article{lee_moser_2019,
  author =        {Lee, Myoungkyu and Moser, Robert D.},
  journal =       {Journal of Fluid Mechanics},
  pages =         {886–938},
  publisher =     {Cambridge University Press},
  title =         {Spectral analysis of the budget equation in turbulent
                   channel flows at high Reynolds number},
  volume =        {860},
  year =          {2019},
  doi =           {10.1017/jfm.2018.903},
}

@misc{Chollet2015,
  author =        {Chollet, Fran\c{c}ois and others},
  howpublished =  {\url{https://keras.io}},
  title =         {Keras},
  year =          {2015},
}

@misc{tensorflow2015-whitepaper,
  author =        {Mart\'{\i}n~Abadi and Ashish~Agarwal and Paul~Barham and
                   Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and
                   Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and
                   Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and
                   Andrew~Harp and Geoffrey~Irving and Michael~Isard and
                   Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and
                   Manjunath~Kudlur and Josh~Levenberg and Dan~Man\'{e} and
                   Rajat~Monga and Sherry~Moore and Derek~Murray and
                   Chris~Olah and Mike~Schuster and Jonathon~Shlens and
                   Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and
                   Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and
                   Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and
                   Martin~Wattenberg and Martin~Wicke and Yuan~Yu and
                   Xiaoqiang~Zheng},
  note =          {Software available from tensorflow.org},
  title =         {{TensorFlow}: Large-Scale Machine Learning on
                   Heterogeneous Systems},
  year =          {2015},
  url =           {http://tensorflow.org/},
}

@article{Ducros1998,
  author =        {Ducros, F. and Nicoud, F. and Poinsot, T.},
  journal =       {Numer. Methods Fluid Dyn. VI},
  pages =         {293--299},
  title =         {{Wall-adapting local eddy-viscosity models for
                   simulations in complex geometries}},
  year =          {1998},
}

@misc{Endres2003,
  author =        {Endres, Dominik M. and Schindelin, Johannes E.},
  booktitle =     {IEEE Trans. Inf. Theory},
  title =         {{A new metric for probability distributions}},
  year =          {2003},
  abstract =      {We introduce a metric for probability distributions,
                   which is bounded, information-theoretically
                   motivated, and has a natural Bayesian interpretation.
                   The square root of the well-known $\chi$2 distance is
                   an asymptotic approximation to it. Moreover, it is a
                   close relative of the capacitory discrimination and
                   Jensen-Shannon divergence.},
  doi =           {10.1109/TIT.2003.813506},
  isbn =          {0018-9448},
  issn =          {00189448},
}

@article{Osterreicher2003,
  author =        {{\"{O}}sterreicher, Ferdinand and Vajda, Igor},
  journal =       {Ann. Inst. Stat. Math.},
  title =         {{A new class of metric divergences on probability
                   spaces and its applicability in statistics}},
  year =          {2003},
  abstract =      {The classI f $\beta$, $\beta$$\epsilon$(0, ∞],
                   off-divergences investigated in this paper is defined
                   in terms of a class of entropies introduced by
                   Arimoto (1971,Information and Control,19, 181–194).
                   It contains the squared Hellinger distance (for
                   $\beta$=1/2), the sumI(Q 1‖(Q 1+Q 2)/2)+I(Q 2‖(Q
                   1+Q 2)/2) of Kullback-Leibler divergences (for
                   $\beta$=1) and half of the variation distance (for
                   $\beta$=∞) and continuously extends the class of
                   squared perimeter-type distances introduced by
                   {\"{O}}sterreicher (1996,Kybernetika,32, 389–393)
                   (for $\beta$$\epsilon$ (1, ∞]). It is shown that
                   (If$\beta$(Q1,Q2))min($\beta$,1/2) are distances of
                   probability distributionsQ 1,Q 2 for $\beta$
                   $\epsilon$ (0, ∞). The applicability of If$\beta$
                   -divergences in statistics is also considered. In
                   particular, it is shown that the If$\beta$
                   -projections of appropriate empirical distributions
                   to regular families define distribution estimates
                   which are in the case of an i.i.d. sample of size'n
                   consistent. The order of consistency is investigated
                   as well.},
  doi =           {10.1007/BF02517812},
  isbn =          {0743-7463},
  issn =          {00203157},
}

@article{Kullback1987,
  author =        {Kullback, S.},
  journal =       {Am. Stat.},
  month =         {nov},
  number =        {4},
  pages =         {338--341},
  title =         {{Letters to the Editor}},
  volume =        {41},
  year =          {1987},
  doi =           {10.1080/00031305.1987.10475510},
  issn =          {0003-1305},
  url =           {http://www.tandfonline.com/doi/abs/10.1080/
                   00031305.1987.10475510},
}

@article{Pedregosa2011,
  author =        {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and
                   Michel, V. and Thirion, B. and Grisel, O. and
                   Blondel, M. and Prettenhofer, P. and Weiss, R. and
                   Dubourg, V. and Vanderplas, J. and Passos, A. and
                   Cournapeau, D. and Brucher, M. and Perrot, M. and
                   Duchesnay, E.},
  journal =       {J. Mach. Learn. Res.},
  title =         {{Scikit-learn: Machine Learning in Python}},
  year =          {2011},
  abstract =      {Cet ouvrage retranscrit cinq entretiens,
                   diffus{\'{e}}s en 1988 sur France Culture entre
                   l'historien Roger Chartier et le sociologue Pierre
                   Bourdieu dont la pens{\'{e}}e, moins largement
                   diffus{\'{e}}e qu'aujourd'hui, {\'{e}}tait
                   d{\'{e}}j{\`{a}} l'objet de r{\'{e}}actions hostiles
                   ou d'interpr{\'{e}}tations r{\'{e}}ductrices. Chaque
                   entretien permit {\`{a}} Bourdieu de d{\'{e}}velopper
                   en direction d'un public {\'{e}}largi ses
                   r{\'{e}}flexions sur des th{\`{e}}mes comme le
                   m{\'{e}}tier de sociologue, illusions et
                   connaissance, structure et individu{\ldots} Sans que
                   soient mises en cause...},
  doi =           {10.1007/s13398-014-0173-7.2},
  isbn =          {9781783281930},
  issn =          {1271-6669},
}

